{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7feb4327-b549-49a8-8995-f5b5317a0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b4cc4b-ed83-4f36-8df8-3f722fe19f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd57f6d-329d-42f8-aefd-6f63d946a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids)- max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0e31eb88-296d-4486-b792-732be8324f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=8, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size,  \n",
    "                            shuffle=shuffle, \n",
    "                            drop_last=drop_last, \n",
    "                            num_workers=num_workers)\n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde34bed-1c96-4f8d-8124-762bf07f7c8a",
   "metadata": {},
   "source": [
    "## Testing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "099ef8c1-a6cf-447a-8590-8f1b3a04be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TrainingData/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e3023597-afb1-49e7-93b3-793863d1a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(text,batch_size=4, max_length=4, stride=4,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1e450-bdd5-412e-96be-35cc9e0e9bb1",
   "metadata": {},
   "source": [
    "# Token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3bbb25da-a0f9-435a-9d26-8f91d6ae73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fox --> 2\n",
    "# jumps --> 3\n",
    "# over --> 5\n",
    "# dog --> 1\n",
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fc119c1f-0398-47b7-a582-e536c23cf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b5131-b7b4-4901-a581-1750baa2cd99",
   "metadata": {},
   "source": [
    "## creating the vector emebedding for the entire vocab size with each words has 512 dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "81f59d31-096a-444f-ae1f-93753f1f2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a3b1f007-45f5-4410-93c6-31f51b060f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.0315, -1.0640,  0.9417],\n",
       "        [-1.3152, -0.0677, -0.1350,  ..., -0.4840, -0.2713, -0.0774],\n",
       "        [ 0.5229,  0.1553,  0.5247,  ..., -0.4098,  0.4978, -0.3721],\n",
       "        ...,\n",
       "        [-0.1187,  0.6496,  0.2482,  ..., -1.0761,  0.7409,  1.9404],\n",
       "        [-0.8611,  1.6253,  1.4460,  ..., -1.1652, -0.6987, -0.3244],\n",
       "        [-0.0217,  0.1480,  0.6482,  ...,  0.7337,  1.7774, -0.5395]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(token_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67092e-aa8e-4f9a-9154-3d29b234cc6c",
   "metadata": {},
   "source": [
    "## creating input-target pairs to feed into llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5520aff8-b1ed-4ae1-9abb-5236b2be8e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]) torch.Size([8, 4])\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(inputs, inputs.shape)\n",
    "print(targets, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64964ad-915a-413d-85ed-4b6fae85ce0f",
   "metadata": {},
   "source": [
    "### now the input-target pairs gets embedded into higher dimensional (512 dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "46e036e0-2bd0-4238-a730-4423c435317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1202c0-ebb3-4cb2-ac15-845a3fc377b9",
   "metadata": {},
   "source": [
    "## each word has embedded into higher dimensional with shape of 8 * 4 * 512, it means 8 batchs of 4 tokens ids in which each token has shape of 512\n",
    "#### at first batches doesn't make any sense, for beginners they get confused with the batches because the matrix dim will be increased from 2 dim to 3 dim now the matrix multiplication will be more devious "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fccf7999-1736-45cb-9fce-2a930f4f4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4973fc-acfc-4c98-8b23-fbdc97e80b81",
   "metadata": {},
   "source": [
    "#### context_length means \"\"the maximum number of tokens the model can \"see\" at once when making predictionst\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3200ad71-c069-4d8c-8fa1-da6484e0ad6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4846,  0.2486, -0.4608,  ...,  0.3308,  1.1665,  0.5254],\n",
       "        [-0.7847,  0.9318, -0.5104,  ..., -0.3187, -0.8253, -0.5695],\n",
       "        [ 0.9040,  0.5810, -0.7173,  ..., -0.3400, -0.7329, -0.8389],\n",
       "        [-1.3137, -1.9783,  1.3846,  ...,  1.1268, -2.1751,  0.1330]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebf40c13-b4c9-428f-9c4f-167bddeba536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 512])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1dede-407a-4f40-b02b-b6bc4ca4472b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
